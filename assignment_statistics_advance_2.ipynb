{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae6c885-cb2b-45e5-ae94-15825a20d042",
   "metadata": {},
   "source": [
    "# ASSIGNMENT STATISTICS ADVANCE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2283dcc6-99cd-4e2e-8aab-99748bf4a142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_1_ANS :- PMF [Probabitity Mass Function] :- The Probability Mass function is defined on all the values of R, where it takes all the arguments of any real number. It doesn’t belong to the value of X when the argument value equals to zero and when the argument belongs to x, the value of PMF should be positive.\n",
      "\n",
      "The probability mass function is usually the primary component of defining a discrete probability distribution, but it differs from the probability density function (PDF) where it produces distinct outcomes. This is the reason why probability mass function is used in computer programming and statistical modelling. In other words, probability mass function is a function that relates discrete events to the probabilities associated with those events occurring. The word “mass“ indicates the probabilities that are concentrated on discrete events.\n",
      "\n",
      "Probability Mass Function Example :-\n",
      "Question:\n",
      "Rolling a dicc\n",
      "{1,2,3,4,5,6}\n",
      "\n",
      "Solution:\n",
      "=> pr(X = 1)\n",
      "pr(x = 2)\n",
      "=> 1/6\n",
      "pr(x = <=2) = pr(x = 1) + pr(x = 2)\n",
      "= 1/6 + 1/6 = 2/6\n",
      "\n",
      "\n",
      "PDF [Probabitity Density Function] :-The Probability Density Function(PDF) defines the probability function representing the density of a continuous random variable lying between a specific range of values. In other words, the probability density function produces the likelihood of values of the continuous random variable. Sometimes it is also called a probability distribution function or just a probability function. However, this function is stated in many other sources as the function over a broad set of values. Often it is referred to as cumulative distribution function or sometimes as probability mass function(PMF). However, the actual truth is PDF (probability density function ) is defined for continuous random variables, whereas PMF (probability mass function) is defined for discrete random variables.\n",
      "\n",
      "Probability Density Function Example :-\n",
      "Question:\n",
      "Let X be a continuous random variable with the PDF given by:\n",
      "Find P(0.5 < x < 1.5).\n",
      "\n",
      "Solution:\n",
      "= [(1)2/2 – (0.5)2/2] + {[2(1.5) – (1.5)2/2] – [2(1) – (1)2/2]}\n",
      "= [(½) – (⅛)] + {[3 – (9/8)] – [2 – (½)]}\n",
      "= (⅜) + [(15/8) – (3/2)]\n",
      "= (3 + 15 – 12)/8\n",
      "= 6/8\n",
      "= 3/4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_1_ANS :- PMF [Probabitity Mass Function] :- The Probability Mass function is defined on all the values of R, where it takes all the arguments of any real number. It doesn’t belong to the value of X when the argument value equals to zero and when the argument belongs to x, the value of PMF should be positive.\\n\\nThe probability mass function is usually the primary component of defining a discrete probability distribution, but it differs from the probability density function (PDF) where it produces distinct outcomes. This is the reason why probability mass function is used in computer programming and statistical modelling. In other words, probability mass function is a function that relates discrete events to the probabilities associated with those events occurring. The word “mass“ indicates the probabilities that are concentrated on discrete events.\\n\\nProbability Mass Function Example :-\\nQuestion:\\nRolling a dicc\\n{1,2,3,4,5,6}\\n\\nSolution:\\n=> pr(X = 1)\\npr(x = 2)\\n=> 1/6\\npr(x = <=2) = pr(x = 1) + pr(x = 2)\\n= 1/6 + 1/6 = 2/6\\n\\n\\nPDF [Probabitity Density Function] :-The Probability Density Function(PDF) defines the probability function representing the density of a continuous random variable lying between a specific range of values. In other words, the probability density function produces the likelihood of values of the continuous random variable. Sometimes it is also called a probability distribution function or just a probability function. However, this function is stated in many other sources as the function over a broad set of values. Often it is referred to as cumulative distribution function or sometimes as probability mass function(PMF). However, the actual truth is PDF (probability density function ) is defined for continuous random variables, whereas PMF (probability mass function) is defined for discrete random variables.\\n\\nProbability Density Function Example :-\\nQuestion:\\nLet X be a continuous random variable with the PDF given by:\\nFind P(0.5 < x < 1.5).\\n\\nSolution:\\n= [(1)2/2 – (0.5)2/2] + {[2(1.5) – (1.5)2/2] – [2(1) – (1)2/2]}\\n= [(½) – (⅛)] + {[3 – (9/8)] – [2 – (½)]}\\n= (⅜) + [(15/8) – (3/2)]\\n= (3 + 15 – 12)/8\\n= 6/8\\n= 3/4 \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34281f67-9d38-4cb4-8ccf-2e8a64354e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_2_ANS :-The cumulative distribution function of a real-valued random variable X is the function given by[2].\n",
      "\n",
      "where the right-hand side represents the probability that the random variable X takes on a value less than or equal to x.\n",
      "\n",
      "The probability that X lies in the semi-closed interval (a,b], where a<b, is therefore[2]: \n",
      "\n",
      "In the definition above, the 'less than or equal to' sign, '≤', is a convention, not a universally used one (e.g. Hungarian literature uses '<'), but the distinction is important for discrete distributions. The proper use of tables of the binomial and Poisson distributions depends upon this convention. Moreover, important formulas like Paul Lévy's inversion formula for the characteristic function also rely on the 'less than or equal' formulation.\n",
      "\n",
      "If treating several random variables X,Y,…{\\displaystyle X,Y,\\ldots } etc. the corresponding letters are used as subscripts while, if treating only one, the subscript is usually omitted.It is conventional to use a capital F for a cumulative distribution function, in contrast to the lower-case f used for probability density functions and probability mass functions.This applies when discussing general distributions: some specific distributions have their own conventional notation, for example the normal distribution uses Φ\\Phi  and Φ  instead of F and f, respectively.\n",
      "\n",
      "The cumulative distribution function (CDF) calculates the cumulative probability for a given x-value. Use the CDF to determine the likelihood that a random observation taken from the population will be less than or equal to a particular value.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_2_ANS :-The cumulative distribution function of a real-valued random variable X is the function given by[2].\\n\\nwhere the right-hand side represents the probability that the random variable X takes on a value less than or equal to x.\\n\\nThe probability that X lies in the semi-closed interval (a,b], where a<b, is therefore[2]: \\n\\nIn the definition above, the 'less than or equal to' sign, '≤', is a convention, not a universally used one (e.g. Hungarian literature uses '<'), but the distinction is important for discrete distributions. The proper use of tables of the binomial and Poisson distributions depends upon this convention. Moreover, important formulas like Paul Lévy's inversion formula for the characteristic function also rely on the 'less than or equal' formulation.\\n\\nIf treating several random variables X,Y,…{\\displaystyle X,Y,\\ldots } etc. the corresponding letters are used as subscripts while, if treating only one, the subscript is usually omitted.It is conventional to use a capital F for a cumulative distribution function, in contrast to the lower-case f used for probability density functions and probability mass functions.This applies when discussing general distributions: some specific distributions have their own conventional notation, for example the normal distribution uses Φ\\Phi  and Φ  instead of F and f, respectively.\\n\\nThe cumulative distribution function (CDF) calculates the cumulative probability for a given x-value. Use the CDF to determine the likelihood that a random observation taken from the population will be less than or equal to a particular value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ddb5d0-e66f-471a-ad90-f78b0a1af62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_3_ANS :- The normal distribution is a commonly used statistical model in various fields, including science, engineering, economics, and finance. Here are some examples of situations where the normal distribution might be used as a model:\n",
      "1. Physical measurements: The normal distribution can be used to model the measurements of physical quantities, such as the height of a population, weight, or blood pressure.\n",
      "\n",
      "2. Test scores: The normal distribution is often used to model test scores in educational research, where the scores are assumed to follow a normal distribution.\n",
      "\n",
      "3.Finance: The normal distribution can be used to model financial data, such as stock prices or interest rates.\n",
      "\n",
      "4. Quality control: The normal distribution can be used to model the variability in the manufacturing process of a product.\n",
      "\n",
      "5. Natural phenomena: The normal distribution can be used to model the distribution of various natural phenomena, such as the size of rainfall or the frequency of earthquakes.\n",
      "\n",
      "The normal distribution is defined by two parameters: the mean and the standard deviation. The mean represents the center of the distribution and determines the location of the peak, while the standard deviation determines the width of the distribution. Specifically, the standard deviation measures the amount of spread or variability around the mean.\n",
      "\n",
      "If the standard deviation is small, the distribution will be narrow and tall, indicating that the values are closely clustered around the mean. If the standard deviation is large, the distribution will be wider and flatter, indicating that the values are more spread out from the mean.\n",
      "\n",
      "Overall, the normal distribution is a useful and versatile model that allows for the analysis of data and the inference of conclusions in a wide range of fields.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_3_ANS :- The normal distribution is a commonly used statistical model in various fields, including science, engineering, economics, and finance. Here are some examples of situations where the normal distribution might be used as a model:\\n1. Physical measurements: The normal distribution can be used to model the measurements of physical quantities, such as the height of a population, weight, or blood pressure.\\n\\n2. Test scores: The normal distribution is often used to model test scores in educational research, where the scores are assumed to follow a normal distribution.\\n\\n3.Finance: The normal distribution can be used to model financial data, such as stock prices or interest rates.\\n\\n4. Quality control: The normal distribution can be used to model the variability in the manufacturing process of a product.\\n\\n5. Natural phenomena: The normal distribution can be used to model the distribution of various natural phenomena, such as the size of rainfall or the frequency of earthquakes.\\n\\nThe normal distribution is defined by two parameters: the mean and the standard deviation. The mean represents the center of the distribution and determines the location of the peak, while the standard deviation determines the width of the distribution. Specifically, the standard deviation measures the amount of spread or variability around the mean.\\n\\nIf the standard deviation is small, the distribution will be narrow and tall, indicating that the values are closely clustered around the mean. If the standard deviation is large, the distribution will be wider and flatter, indicating that the values are more spread out from the mean.\\n\\nOverall, the normal distribution is a useful and versatile model that allows for the analysis of data and the inference of conclusions in a wide range of fields.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63b5cdb-84e2-442b-9e7c-e9f0f9a6b891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_4_ANS :- The normal distribution is one of the most important and widely used probability distributions in statistics. It is important for a number of reasons, including:\n",
      "\n",
      "1. Universal applicability: The normal distribution is used to model a wide variety of phenomena in the natural and social sciences, as well as in engineering, finance, and many other fields.\n",
      "\n",
      "2. Central limit theorem: The normal distribution arises naturally as a result of the central limit theorem, which states that the sum of a large number of independent and identically distributed random variables tends to follow a normal distribution, regardless of the distribution of the individual variables.\n",
      "\n",
      "3. Statistical inference: Many statistical methods, such as hypothesis testing, confidence intervals, and regression analysis, rely on the assumption of normality, making the normal distribution a fundamental tool in statistical inference.\n",
      "\n",
      "Real-life examples of normal distribution include:\n",
      "\n",
      "1. Human height: The heights of a large population of people tend to follow a normal distribution, with the mean height being around 5'7' for adult males in the United States.\n",
      "\n",
      "2. Exam scores: Scores on standardized exams, such as the SAT or ACT, often follow a normal distribution, with the mean score being around 500.\n",
      "\n",
      "3. IQ scores: IQ scores are designed to follow a normal distribution, with a mean of 100 and a standard deviation of 15.\n",
      "\n",
      "4. Body weight: The weight of a large population of people tends to follow a normal distribution, with the mean weight being around 170 pounds for adult males in the United States.\n",
      "\n",
      "5. Blood pressure: Blood pressure readings in a population tend to follow a normal distribution, with the mean systolic blood pressure being around 120 mmHg and the mean diastolic blood pressure being around 80 mmHg.\n",
      "\n",
      "Overall, the normal distribution plays a crucial role in statistical analysis and provides a useful framework for modeling a wide range of natural and social phenomena.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_4_ANS :- The normal distribution is one of the most important and widely used probability distributions in statistics. It is important for a number of reasons, including:\\n\\n1. Universal applicability: The normal distribution is used to model a wide variety of phenomena in the natural and social sciences, as well as in engineering, finance, and many other fields.\\n\\n2. Central limit theorem: The normal distribution arises naturally as a result of the central limit theorem, which states that the sum of a large number of independent and identically distributed random variables tends to follow a normal distribution, regardless of the distribution of the individual variables.\\n\\n3. Statistical inference: Many statistical methods, such as hypothesis testing, confidence intervals, and regression analysis, rely on the assumption of normality, making the normal distribution a fundamental tool in statistical inference.\\n\\nReal-life examples of normal distribution include:\\n\\n1. Human height: The heights of a large population of people tend to follow a normal distribution, with the mean height being around 5'7' for adult males in the United States.\\n\\n2. Exam scores: Scores on standardized exams, such as the SAT or ACT, often follow a normal distribution, with the mean score being around 500.\\n\\n3. IQ scores: IQ scores are designed to follow a normal distribution, with a mean of 100 and a standard deviation of 15.\\n\\n4. Body weight: The weight of a large population of people tends to follow a normal distribution, with the mean weight being around 170 pounds for adult males in the United States.\\n\\n5. Blood pressure: Blood pressure readings in a population tend to follow a normal distribution, with the mean systolic blood pressure being around 120 mmHg and the mean diastolic blood pressure being around 80 mmHg.\\n\\nOverall, the normal distribution plays a crucial role in statistical analysis and provides a useful framework for modeling a wide range of natural and social phenomena.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f61ad9b0-4968-4e1f-b4d7-73dc3ba6ac2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_5_ANS :- In probability theory and statistics, the Bernoulli distribution, named after Swiss mathematician Jacob Bernoulli,[1] is the discrete probability distribution of a random variable which takes the value 1 with probability p and the value 0 with probability q=1-p. Less formally, it can be thought of as a model for the set of possible outcomes of any single experiment that asks a yes–no question. \n",
      "\n",
      "Difference :\n",
      "\n",
      "1. Bernoulli Distribution:\n",
      "This distribution deals with the data which only has 1 trial & only 2 possible outcomes. Anything other than that will not fall under the Bernoulli Distribution category.\n",
      "\n",
      "Way to represent Bernoulli Trial/Event:\n",
      "\n",
      "Although anyone can use any symbol to represent the distribution for the sake of simplification, it is been represented as:\n",
      "\n",
      "Brenaulli(p)\n",
      "\n",
      "In the above image, “p” represents the probability of the event to occur; For example, the probability of getting heads in a coin toss.\n",
      "\n",
      "Example:\n",
      "\n",
      "For the real-life example, let’s consider the situation of passing or failing an exam. Let’s assume the probability to pass the exam is 95%, therefore the probability to fail will be 5%.\n",
      "\n",
      "In this case, if the event to pass the exam is considered, then the Bernoulli event will contain the probability of passing the exam. Similarly, it goes for failing the exam.\n",
      "\n",
      "Bernoulli Keypoints:\n",
      "\n",
      "Bernoulli deals with the outcome of the single trial of the event, whereas Binomial deals with the outcome of the multiple trials of the single event.\n",
      "\n",
      "\n",
      "2. Binomial Distribution:\n",
      "\n",
      "It is the collection of Bernoulli trials for the same event, i.e., it contains more than 1 Bernoulli event for the same scenario for which the Bernoulli trial is calculated.\n",
      "\n",
      "Way to represent Binomial Distribution\n",
      "It can be represented using two things:\n",
      "\n",
      "1. The number of Bernoulli trials.\n",
      "\n",
      "2. Probability of an event in each trial.\n",
      "\n",
      "Binomial Distribution(n,p)\n",
      "\n",
      "In the image above, “n” corresponds to the number of Bernoulli trials, & “p” corresponds to the probability of the event in each trial.\n",
      "\n",
      "Example of Binomial Distribution:\n",
      "\n",
      "Considering the same example of the Bernoulli Distribution, let’s create Binomial Distribution from that example.\n",
      "\n",
      "Considering 95% & 5% for passing & failing an exam for a student respectively. If we want to calculate the probability of a student to pass exactly 5 exams out of 5 exams in which it appeared, using the above probability formula it can be easily calculated.\n",
      "\n",
      "Binomial Keypoints:\n",
      "\n",
      "Bernoulli is used when the outcome of an event is required for only one time, whereas the Binomial is used when the outcome of an event is required multiple times.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_5_ANS :- In probability theory and statistics, the Bernoulli distribution, named after Swiss mathematician Jacob Bernoulli,[1] is the discrete probability distribution of a random variable which takes the value 1 with probability p and the value 0 with probability q=1-p. Less formally, it can be thought of as a model for the set of possible outcomes of any single experiment that asks a yes–no question. \\n\\nDifference :\\n\\n1. Bernoulli Distribution:\\nThis distribution deals with the data which only has 1 trial & only 2 possible outcomes. Anything other than that will not fall under the Bernoulli Distribution category.\\n\\nWay to represent Bernoulli Trial/Event:\\n\\nAlthough anyone can use any symbol to represent the distribution for the sake of simplification, it is been represented as:\\n\\nBrenaulli(p)\\n\\nIn the above image, “p” represents the probability of the event to occur; For example, the probability of getting heads in a coin toss.\\n\\nExample:\\n\\nFor the real-life example, let’s consider the situation of passing or failing an exam. Let’s assume the probability to pass the exam is 95%, therefore the probability to fail will be 5%.\\n\\nIn this case, if the event to pass the exam is considered, then the Bernoulli event will contain the probability of passing the exam. Similarly, it goes for failing the exam.\\n\\nBernoulli Keypoints:\\n\\nBernoulli deals with the outcome of the single trial of the event, whereas Binomial deals with the outcome of the multiple trials of the single event.\\n\\n\\n2. Binomial Distribution:\\n\\nIt is the collection of Bernoulli trials for the same event, i.e., it contains more than 1 Bernoulli event for the same scenario for which the Bernoulli trial is calculated.\\n\\nWay to represent Binomial Distribution\\nIt can be represented using two things:\\n\\n1. The number of Bernoulli trials.\\n\\n2. Probability of an event in each trial.\\n\\nBinomial Distribution(n,p)\\n\\nIn the image above, “n” corresponds to the number of Bernoulli trials, & “p” corresponds to the probability of the event in each trial.\\n\\nExample of Binomial Distribution:\\n\\nConsidering the same example of the Bernoulli Distribution, let’s create Binomial Distribution from that example.\\n\\nConsidering 95% & 5% for passing & failing an exam for a student respectively. If we want to calculate the probability of a student to pass exactly 5 exams out of 5 exams in which it appeared, using the above probability formula it can be easily calculated.\\n\\nBinomial Keypoints:\\n\\nBernoulli is used when the outcome of an event is required for only one time, whereas the Binomial is used when the outcome of an event is required multiple times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06a93bed-635d-4a8e-be02-e2ed0b2c22de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_6_ANS :- formula : Xi - μ / σ \n",
      "\n",
      "Xi = 60 \n",
      "μ = 50\n",
      "σ = 10\n",
      "\n",
      "60 - 50 \n",
      "= 10\n",
      "\n",
      "10/15\n",
      "=0.6\n",
      "\n",
      "Area under the curve(<60) = 0.7237 = 72.57%\n",
      "\n",
      " 0.7257\n",
      "-\n",
      " 1.0000\n",
      "\n",
      "=0.2743\n",
      "\n",
      "Area under the curve(60>) = 0.2743 = 27.43%\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_6_ANS :- formula : Xi - μ / σ \\n\\nXi = 60 \\nμ = 50\\nσ = 10\\n\\n60 - 50 \\n= 10\\n\\n10/15\\n=0.6\\n\\nArea under the curve(<60) = 0.7237 = 72.57%\\n\\n 0.7257\\n-\\n 1.0000\\n\\n=0.2743\\n\\nArea under the curve(60>) = 0.2743 = 27.43%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb2bbb2-5be4-4d66-9a24-fe01cee7e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_7_ANS :- In statistics, uniform distribution refers to a type of probability distribution in which all outcomes are equally likely. A deck of cards has within it uniform distributions because the likelihood of drawing a heart, a club, a diamond, or a spade is equally likely. A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same.\n",
      "\n",
      "Two type of dustribution:\n",
      "\n",
      "1. Continuos Uniform Dustribution\n",
      "\n",
      "2. Discrite Uniform Dustribution\n",
      "\n",
      "1. Continuos Uniform Dustribution:\n",
      "\n",
      "In probability theory and statistics, the continuous uniform distributions or rectangular distributions are a family of symmetric probability distributions. Such a distribution describes an experiment where there is an arbitrary outcome that lies between certain bounds.[1] The bounds are defined by the parameters, a and b, which are the minimum and maximum values.\n",
      "\n",
      "Notation : U(a,b)\n",
      "\n",
      "Example :Probability of daly scale to fall between 15 and 30.\n",
      "\n",
      "P(15 <= X <= 30)\n",
      "Xi = 15\n",
      "X2 = 30\n",
      "= 30 - 15 * 1 / 30\n",
      "= 15 * 1/ 30\n",
      "= 0.5 = 50% \n",
      "\n",
      "\n",
      " 2. Discrite Uniform Dustribution :\n",
      "\n",
      "In probability theory and statistics, the discrete uniform distribution is a symmetric probability distribution wherein a finite number of values are equally likely to be observed; every one of n values has equal probability 1/n. Another way of saying 'discrete uniform distribution' would be 'a known, finite number of outcomes equally likely to happen'.\n",
      "\n",
      "Notation : U(a,b)\n",
      "\n",
      "Example : Rolling a dice :\n",
      "{1,2,3,4,5,6}\n",
      "\n",
      "a = 1\n",
      "b = 6\n",
      "\n",
      "pr(1) = 1/6\n",
      "pr(2) = 1/6\n",
      "pr(3) = 1/6\n",
      "npr(4) = 1/6\n",
      "npr(5) = 1/6\n",
      "pr(6) = 1/6\n",
      "\n",
      "n = b - a + 1\n",
      "\n",
      "n = 6 - 1 + 1 = 6\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_7_ANS :- In statistics, uniform distribution refers to a type of probability distribution in which all outcomes are equally likely. A deck of cards has within it uniform distributions because the likelihood of drawing a heart, a club, a diamond, or a spade is equally likely. A coin also has a uniform distribution because the probability of getting either heads or tails in a coin toss is the same.\\n\\nTwo type of dustribution:\\n\\n1. Continuos Uniform Dustribution\\n\\n2. Discrite Uniform Dustribution\\n\\n1. Continuos Uniform Dustribution:\\n\\nIn probability theory and statistics, the continuous uniform distributions or rectangular distributions are a family of symmetric probability distributions. Such a distribution describes an experiment where there is an arbitrary outcome that lies between certain bounds.[1] The bounds are defined by the parameters, a and b, which are the minimum and maximum values.\\n\\nNotation : U(a,b)\\n\\nExample :Probability of daly scale to fall between 15 and 30.\\n\\nP(15 <= X <= 30)\\nXi = 15\\nX2 = 30\\n= 30 - 15 * 1 / 30\\n= 15 * 1/ 30\\n= 0.5 = 50% \\n\\n\\n 2. Discrite Uniform Dustribution :\\n\\nIn probability theory and statistics, the discrete uniform distribution is a symmetric probability distribution wherein a finite number of values are equally likely to be observed; every one of n values has equal probability 1/n. Another way of saying 'discrete uniform distribution' would be 'a known, finite number of outcomes equally likely to happen'.\\n\\nNotation : U(a,b)\\n\\nExample : Rolling a dice :\\n{1,2,3,4,5,6}\\n\\na = 1\\nb = 6\\n\\npr(1) = 1/6\\npr(2) = 1/6\\npr(3) = 1/6\\nnpr(4) = 1/6\\nnpr(5) = 1/6\\npr(6) = 1/6\\n\\nn = b - a + 1\\n\\nn = 6 - 1 + 1 = 6\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f39ee20-1344-4f89-baca-c33fa0844e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_8_ANS :- A z-score can be placed on a normal distribution curve. Z-scores range from -3 standard deviations (which would fall to the far left of the normal distribution curve) up to +3 standard deviations (which would fall to the far right of the normal distribution curve). In order to use a z-score, you need to know the mean μ and also the population standard deviation σ.\n",
      "\n",
      "Z-scores are a way to compare results to a “normal” population. Results from tests or surveys have thousands of possible results and units; those results can often seem meaningless. For example, knowing that someone’s weight is 150 pounds might be good information, but if you want to compare it to the “average” person’s weight, looking at a vast table of data can be overwhelming (especially if some weights are recorded in kilograms). A z-score can tell you where that person’s weight is compared to the average population’s mean weight.\n",
      "\n",
      "Z Score Formulas\n",
      "\n",
      " z = (x – μ) / σ\n",
      "\n",
      "The z-score in the center of the curve is zero. The z-scores to the right of the mean are positive and the z-scores to the left of the mean are negative. If you look up the score in the z-table, you can tell what percentage of the population is above or below your score. The table below shows a z-score of 2.0 highlighted, showing .9772 (which converts to 97.72%). If you look at the same score (2.0) of the normal distribution curve above, you’ll see it corresponds with 97.72%.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_8_ANS :- A z-score can be placed on a normal distribution curve. Z-scores range from -3 standard deviations (which would fall to the far left of the normal distribution curve) up to +3 standard deviations (which would fall to the far right of the normal distribution curve). In order to use a z-score, you need to know the mean μ and also the population standard deviation σ.\\n\\nZ-scores are a way to compare results to a “normal” population. Results from tests or surveys have thousands of possible results and units; those results can often seem meaningless. For example, knowing that someone’s weight is 150 pounds might be good information, but if you want to compare it to the “average” person’s weight, looking at a vast table of data can be overwhelming (especially if some weights are recorded in kilograms). A z-score can tell you where that person’s weight is compared to the average population’s mean weight.\\n\\nZ Score Formulas\\n\\n z = (x – μ) / σ\\n\\nThe z-score in the center of the curve is zero. The z-scores to the right of the mean are positive and the z-scores to the left of the mean are negative. If you look up the score in the z-table, you can tell what percentage of the population is above or below your score. The table below shows a z-score of 2.0 highlighted, showing .9772 (which converts to 97.72%). If you look at the same score (2.0) of the normal distribution curve above, you’ll see it corresponds with 97.72%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83498611-5c7f-40c6-9788-ba19bed5c6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_9_ANS :- In probability theory, the central limit theorem (CLT) establishes that, in many situations, for identically distributed independent samples, the standardized sample mean tends towards the standard normal distribution even if the original variables themselves are not normally distributed.\n",
      "\n",
      "The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.\n",
      "\n",
      "This theorem has seen many changes during the formal development of probability theory. Previous versions of the theorem date back to 1811, but in its modern general form, this fundamental result in probability theory was precisely stated as late as 1920,[1] thereby serving as a bridge between classical and modern probability theory.\n",
      "\n",
      "If X1,X2,…,Xn,… are random samples drawn from a population with overall mean μ and finite variance σ2, and if Xn is the sample mean of the first n samples, then the limiting form of the distribution, Z=lim n→∞(Xn−μ/σ2 , with σx=σ/root(n) , is a standard normal distribution.[2]\n",
      "\n",
      "For example, suppose that a sample is obtained containing many observations, each observation being randomly generated in a way that does not depend on the values of the other observations, and that the arithmetic mean of the observed values is computed. If this procedure is performed many times, the central limit theorem says that the probability distribution of the average will closely approximate a normal distribution.\n",
      "\n",
      "he central limit theorem has several variants. In its common form, the random variables must be independent and identically distributed (i.i.d.). In variants, convergence of the mean to the normal distribution also occurs for non-identical distributions or for non-independent observations, if they comply with certain conditions.\n",
      "\n",
      "The earliest version of this theorem, that the normal distribution may be used as an approximation to the binomial distribution, is the de Moivre–Laplace theorem.\n"
     ]
    }
   ],
   "source": [
    "print(\"Q_9_ANS :- In probability theory, the central limit theorem (CLT) establishes that, in many situations, for identically distributed independent samples, the standardized sample mean tends towards the standard normal distribution even if the original variables themselves are not normally distributed.\\n\\nThe theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.\\n\\nThis theorem has seen many changes during the formal development of probability theory. Previous versions of the theorem date back to 1811, but in its modern general form, this fundamental result in probability theory was precisely stated as late as 1920,[1] thereby serving as a bridge between classical and modern probability theory.\\n\\nIf X1,X2,…,Xn,… are random samples drawn from a population with overall mean μ and finite variance σ2, and if Xn is the sample mean of the first n samples, then the limiting form of the distribution, Z=lim n→∞(Xn−μ/σ2 , with σx=σ/root(n) , is a standard normal distribution.[2]\\n\\nFor example, suppose that a sample is obtained containing many observations, each observation being randomly generated in a way that does not depend on the values of the other observations, and that the arithmetic mean of the observed values is computed. If this procedure is performed many times, the central limit theorem says that the probability distribution of the average will closely approximate a normal distribution.\\n\\nhe central limit theorem has several variants. In its common form, the random variables must be independent and identically distributed (i.i.d.). In variants, convergence of the mean to the normal distribution also occurs for non-identical distributions or for non-independent observations, if they comply with certain conditions.\\n\\nThe earliest version of this theorem, that the normal distribution may be used as an approximation to the binomial distribution, is the de Moivre–Laplace theorem.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113f764-3dd5-4d66-b6ed-11e0125bbff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Q_10_ANS :- \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
